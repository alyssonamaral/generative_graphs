from __future__ import annotations

import argparse
from pathlib import Path
import time
import torch

from rlod.utils.io import load_pickle
from rlod.graphs.types import GraphSample
from rlod.rl.policy import GraphOrderingPolicy
from rlod.rl.reward import DGMGNLLReward
from rlod.rl.algos.reinforce import ReinforceTrainer, ReinforceConfig


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser()
    p.add_argument("--data_path", type=str, required=True)
    p.add_argument("--dgmg_ckpt", type=str, required=True)

    p.add_argument("--device", type=str, default="cpu")
    p.add_argument("--seed", type=int, default=0)

    p.add_argument("--steps", type=int, default=5000)
    p.add_argument("--print_every", type=int, default=200)
    p.add_argument("--save_every", type=int, default=1000)

    p.add_argument("--lr", type=float, default=3e-4)
    p.add_argument("--entropy_beta", type=float, default=1e-3)
    p.add_argument("--grad_clip", type=float, default=1.0)
    p.add_argument("--baseline_ema", type=float, default=0.95)

    p.add_argument("--nll_scale", type=float, default=1.0)
    p.add_argument("--ckpt_dir", type=str, default="runs/rl_dgmg")
    return p.parse_args()


def main() -> None:
    args = parse_args()
    torch.manual_seed(args.seed)

    raw = load_pickle(args.data_path)
    samples = [GraphSample.from_dict(d) for d in raw]

    policy = GraphOrderingPolicy(emb_dim=64, state_dim=64)

    cfg = ReinforceConfig(
        lr=args.lr,
        entropy_beta=args.entropy_beta,
        grad_clip=args.grad_clip,
        baseline_ema=args.baseline_ema,
    )
    trainer = ReinforceTrainer(policy, cfg=cfg, device=args.device)

    reward_fn = DGMGNLLReward(
        ckpt_path=args.dgmg_ckpt,
        device=args.device,
        nll_scale=args.nll_scale,
    )

    ckpt_dir = Path(args.ckpt_dir)
    ckpt_dir.mkdir(parents=True, exist_ok=True)

    t0 = time.time()
    alpha = 0.98
    ema_r = None

    for step in range(1, args.steps + 1):
        s = samples[step % len(samples)]
        adj = torch.tensor(s.adj, dtype=torch.float32, device=trainer.device)

        out = trainer.policy.sample_order(adj)
        r = reward_fn(s, out.order).to(trainer.device)

        stats = trainer.update(r, out.logprob_sum, out.entropy_sum)

        ema_r = stats["reward"] if ema_r is None else alpha * ema_r + (1 - alpha) * stats["reward"]

        if step % args.print_every == 0:
            dt = time.time() - t0
            print(
                f"[{step:6d}] loss={stats['loss']:.3f} "
                f"R={stats['reward']:.2f} emaR={ema_r:.2f} "
                f"adv={stats['adv']:.2f} ent={stats['entropy_sum']:.2f} dt={dt:.1f}s"
            )

        if step % args.save_every == 0:
            path = ckpt_dir / f"policy_step{step}.pt"
            torch.save(
                {"step": step, "policy_state": trainer.policy.state_dict(), "cfg": cfg.__dict__, "seed": args.seed},
                path,
            )
            print(f"Saved: {path}")

    final_path = ckpt_dir / "policy_final.pt"
    torch.save(
        {"step": args.steps, "policy_state": trainer.policy.state_dict(), "cfg": cfg.__dict__, "seed": args.seed},
        final_path,
    )
    print(f"Saved final: {final_path}")


if __name__ == "__main__":
    main()
